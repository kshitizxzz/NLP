{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f550fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a42ed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/aviral/Desktop/NLP PRACTICAL/IMDB Dataset 3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8723e81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af2b63d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.<br /><br />ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][3].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36a0f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d211c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def striptml(data):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('' , data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "792e48cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(striptml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f91530be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. the filming tec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. the filming tec...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8669f584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e40a307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'https://,www.com hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e588c4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' hello world'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_url(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c1002c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string , time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf61d6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94078c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0be0767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    for char in exclude :\n",
    "        text = text.replace(char , '')\n",
    "    return text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6169a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '*$&%hello*$!ji kese hai. app ??'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b903238c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7179718017578125e-05"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "remove_punc(text)\n",
    "t2 = time.time()-t1\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f228383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_p(text):\n",
    "    return text.translate(str.maketrans('','',exclude))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc6ff69f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.557868957519531e-05"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "remove_p(text)\n",
    "t3 = time.time() - t1\n",
    "t3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f0a9baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /opt/anaconda3/lib/python3.12/site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in /opt/anaconda3/lib/python3.12/site-packages (from textblob) (3.9.2)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk>=3.9->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from nltk>=3.9->textblob) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from nltk>=3.9->textblob) (4.66.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24ba5107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hellloooo how are you'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_text = 'hellloooo howw aree yoou'\n",
    "textblb = TextBlob(incorrect_text)\n",
    "textblb.correct().string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31227938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a0979d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e7e0a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_text(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words():\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return ' '.join(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c2d7979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'        house'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'a the hello is at the on my house'\n",
    "remove_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51c590bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'        house'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'a the hello is at the on my house'\n",
    "remove_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a5ee123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'        house helllo  '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'a the hello is at the on my house helllo why not'\n",
    "remove_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf016973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c010c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello my name is kshitiz jkf '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji('hello my name is kshitiz ðŸ˜‚ðŸ˜­jkf ðŸ—¿')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d83a809f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /opt/anaconda3/lib/python3.12/site-packages (2.15.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install emoji\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46e5b91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python is :face_with_tears_of_joy:\n"
     ]
    }
   ],
   "source": [
    "print (emoji.demojize('python is ðŸ˜‚'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a663d501",
   "metadata": {},
   "source": [
    "#### splitting text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47232f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello!', 'i', 'am', 'going', 'to', 'new', 'Delhi!']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using split function\n",
    "\n",
    "sent1 = 'hello! i am going to new Delhi!'\n",
    "text = sent1.split()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8c31036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello ', ' this is my new car .']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split only on basis of a particular thing\n",
    "sent2 = 'hello , this is my new car .'\n",
    "sent2.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d07e8def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using nltk library\n",
    "from nltk.tokenize import word_tokenize , sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "abfa9813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', '!', 'i', 'am', 'going', 'to', 'new', 'Delhi', '!']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = 'hello! i am going to new Delhi!'\n",
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ee0620a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'have', 'a', 'Ph.D', 'in', 'A.I']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent5 = 'I have a Ph.D in A.I'\n",
    "sent6 = \"We're here to help! mail us at nks@gmail.com\"\n",
    "sent7 = 'A 5km ride cost $10.50'\n",
    "\n",
    "word_tokenize(sent5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c58a174e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " \"'re\",\n",
       " 'here',\n",
       " 'to',\n",
       " 'help',\n",
       " '!',\n",
       " 'mail',\n",
       " 'us',\n",
       " 'at',\n",
       " 'nks',\n",
       " '@',\n",
       " 'gmail.com']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent6)\n",
    "# email id also got separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a1904ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', '5km', 'ride', 'cost', '$', '10.50']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e5ba97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.12\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3bef67ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/__init__.py:13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, prefer_gpu, require_cpu, require_gpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/pipeline/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattributeruler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AttributeRuler\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdep_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DependencyParser\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01medit_tree_lemmatizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EditTreeLemmatizer\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/pipeline/attributeruler.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Language\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Matcher\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Scorer\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/language.py:46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BASE_EXCEPTIONS, URL_MATCH\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlookups\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_lookups\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipe_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m analyze_pipes, print_pipe_analysis, validate_attrs\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschemas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     48\u001b[0m     ConfigSchema,\n\u001b[1;32m     49\u001b[0m     ConfigSchemaInit,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m     validate_init_settings,\n\u001b[1;32m     53\u001b[0m )\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Scorer\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/pipe_analysis.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwasabi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m msg\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc, Span, Token\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dot_to_dict\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# This lets us add type hints for mypy etc. without causing circular imports\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/tokens/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_serialize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocBin\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmorphanalysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MorphAnalysis\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/tokens/_serialize.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleFrozenList, ensure_path\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vocab\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dict_proxies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpanGroups\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DOCBIN_ALL_ATTRS \u001b[38;5;28;01mas\u001b[39;00m ALL_ATTRS\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/vocab.pyx:1\u001b[0m, in \u001b[0;36minit spacy.vocab\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/tokens/doc.pyx:49\u001b[0m, in \u001b[0;36minit spacy.tokens.doc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/schemas.py:195\u001b[0m\n\u001b[1;32m    191\u001b[0m         obj \u001b[38;5;241m=\u001b[39m converted\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m validate(TokenPatternSchema, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpattern\u001b[39m\u001b[38;5;124m\"\u001b[39m: obj})\n\u001b[0;32m--> 195\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTokenPatternString\u001b[39;00m(BaseModel):\n\u001b[1;32m    196\u001b[0m     REGEX: Optional[Union[StrictStr, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenPatternString\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m Field(\u001b[38;5;28;01mNone\u001b[39;00m, alias\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    197\u001b[0m     IN: Optional[List[StrictStr]] \u001b[38;5;241m=\u001b[39m Field(\u001b[38;5;28;01mNone\u001b[39;00m, alias\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/main.py:286\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[0;34m(mcs, name, bases, namespace, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__signature__ \u001b[38;5;241m=\u001b[39m ClassAttribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__signature__\u001b[39m\u001b[38;5;124m'\u001b[39m, generate_model_signature(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m, fields, config))\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolve_forward_refs:\n\u001b[0;32m--> 286\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__try_update_forward_refs__()\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# preserve `__set_name__` protocol defined in https://peps.python.org/pep-0487\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# for attributes not in `new_namespace` (e.g. private attributes)\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, obj \u001b[38;5;129;01min\u001b[39;00m namespace\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/main.py:808\u001b[0m, in \u001b[0;36mBaseModel.__try_update_forward_refs__\u001b[0;34m(cls, **localns)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__try_update_forward_refs__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlocalns: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    804\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;124;03m    Same as update_forward_refs but will not raise exception\u001b[39;00m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;124;03m    when forward references are not defined.\u001b[39;00m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 808\u001b[0m     update_model_forward_refs(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__fields__\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__config__\u001b[38;5;241m.\u001b[39mjson_encoders, localns, (\u001b[38;5;167;01mNameError\u001b[39;00m,))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/typing.py:554\u001b[0m, in \u001b[0;36mupdate_model_forward_refs\u001b[0;34m(model, fields, json_encoders, localns, exc_to_suppress)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields:\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 554\u001b[0m         update_field_forward_refs(f, globalns\u001b[38;5;241m=\u001b[39mglobalns, localns\u001b[38;5;241m=\u001b[39mlocalns)\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exc_to_suppress:\n\u001b[1;32m    556\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/typing.py:529\u001b[0m, in \u001b[0;36mupdate_field_forward_refs\u001b[0;34m(field, globalns, localns)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39msub_fields:\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sub_f \u001b[38;5;129;01min\u001b[39;00m field\u001b[38;5;241m.\u001b[39msub_fields:\n\u001b[0;32m--> 529\u001b[0m         update_field_forward_refs(sub_f, globalns\u001b[38;5;241m=\u001b[39mglobalns, localns\u001b[38;5;241m=\u001b[39mlocalns)\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39mdiscriminator_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     field\u001b[38;5;241m.\u001b[39mprepare_discriminated_union_sub_fields()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/typing.py:520\u001b[0m, in \u001b[0;36mupdate_field_forward_refs\u001b[0;34m(field, globalns, localns)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39mtype_\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;241m==\u001b[39m ForwardRef:\n\u001b[1;32m    519\u001b[0m     prepare \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     field\u001b[38;5;241m.\u001b[39mtype_ \u001b[38;5;241m=\u001b[39m evaluate_forwardref(field\u001b[38;5;241m.\u001b[39mtype_, globalns, localns \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39mouter_type_\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;241m==\u001b[39m ForwardRef:\n\u001b[1;32m    522\u001b[0m     prepare \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/typing.py:66\u001b[0m, in \u001b[0;36mevaluate_forwardref\u001b[0;34m(type_, globalns, localns)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_forwardref\u001b[39m(type_: ForwardRef, globalns: Any, localns: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Even though it is the right signature for python 3.9, mypy complains with\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# `error: Too many arguments for \"_evaluate\" of \"ForwardRef\"` hence the cast...\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Any, type_)\u001b[38;5;241m.\u001b[39m_evaluate(globalns, localns, \u001b[38;5;28mset\u001b[39m())\n",
      "\u001b[0;31mTypeError\u001b[0m: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "61022933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f1e85d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35f74c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(text):\n",
    "    return ' '.join([ ps.stem(word) for word in text.split() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b49929cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent5 = 'walk walks walked walking'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "359efe25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_words(sent5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "24f4e557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probabl'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_words('probably')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d88daeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0ae66f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2524da35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948bf20c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a2882b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be66067b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
